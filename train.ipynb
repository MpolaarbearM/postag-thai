{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\postag-thai\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PR': 1, 'PS': 2, 'NU': 3, 'AV': 4, 'XX': 5, 'FX': 6, 'AJ': 7, 'NN': 8, 'AX': 9, 'PU': 10, 'IJ': 11, 'VV': 12, 'CC': 13, 'CL': 14, 'PA': 15, 'NG': 16}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Read dictionary pkl file\n",
    "with open('tag_id.pkl', 'rb') as fp:\n",
    "    tag_id = pickle.load(fp)\n",
    "    print(tag_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None),\n",
       " 'pos_tag': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'token': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_thai = load_dataset(\"json\", data_files=\"thai10_dataset_fixed.json\")\n",
    "\n",
    "pos_tag_thai['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "features = pos_tag_thai['train'].features.copy()\n",
    "features[\"pos_tag\"] = ClassLabel(names=['PR', 'PS', 'NU', 'AV', 'XX', 'FX', 'AJ', 'NN', 'AX', 'PU', 'IJ', 'VV', 'CC', 'CL', 'PA', 'NG'])\n",
    "# def adjust_labels(batch):\n",
    "#     batch[\"pos_tag\"] = [sentiment + 1 for sentiment in batch[\"pos_tag\"]]\n",
    "#     return batch\n",
    "# dataset = pos_tag_thai.map(adjust_labels, batched=True, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None),\n",
       " 'pos_tag': ClassLabel(names=['PR', 'PS', 'NU', 'AV', 'XX', 'FX', 'AJ', 'NN', 'AX', 'PU', 'IJ', 'VV', 'CC', 'CL', 'PA', 'NG'], id=None),\n",
       " 'token': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset:   0%|          | 0/130560 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid string class label NN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\projects\\postag-thai\\train.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/postag-thai/train.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m ClassLabel,Sequence\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/projects/postag-thai/train.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pos_tag_thai[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mcast_column(\u001b[39m\"\u001b[39;49m\u001b[39mpos_tag\u001b[39;49m\u001b[39m\"\u001b[39;49m, Sequence(ClassLabel(names\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m,\u001b[39m4\u001b[39;49m,\u001b[39m5\u001b[39;49m,\u001b[39m6\u001b[39;49m,\u001b[39m7\u001b[39;49m,\u001b[39m8\u001b[39;49m,\u001b[39m9\u001b[39;49m,\u001b[39m10\u001b[39;49m,\u001b[39m11\u001b[39;49m,\u001b[39m12\u001b[39;49m,\u001b[39m13\u001b[39;49m,\u001b[39m14\u001b[39;49m,\u001b[39m15\u001b[39;49m,\u001b[39m16\u001b[39;49m])))\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2110\u001b[0m, in \u001b[0;36mDataset.cast_column\u001b[1;34m(self, column, feature, new_fingerprint)\u001b[0m\n\u001b[0;32m   2108\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\n\u001b[0;32m   2109\u001b[0m features[column] \u001b[39m=\u001b[39m feature\n\u001b[1;32m-> 2110\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcast(features)\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2055\u001b[0m, in \u001b[0;36mDataset.cast\u001b[1;34m(self, features, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, num_proc)\u001b[0m\n\u001b[0;32m   2053\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_format(\u001b[39m\"\u001b[39m\u001b[39marrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2054\u001b[0m \u001b[39m# capture the PyArrow version here to make the lambda serializable on Windows\u001b[39;00m\n\u001b[1;32m-> 2055\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m   2056\u001b[0m     partial(table_cast, schema\u001b[39m=\u001b[39;49mschema),\n\u001b[0;32m   2057\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2058\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2059\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   2060\u001b[0m     load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   2061\u001b[0m     cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   2062\u001b[0m     writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   2063\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   2064\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   2065\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCasting the dataset\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2066\u001b[0m )\n\u001b[0;32m   2067\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mwith_format(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mformat\u001b[39m)\n\u001b[0;32m   2068\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[0;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3474\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3470\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   3471\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[0;32m   3472\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3473\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3474\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   3475\u001b[0m         batch,\n\u001b[0;32m   3476\u001b[0m         indices,\n\u001b[0;32m   3477\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   3478\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[0;32m   3479\u001b[0m     )\n\u001b[0;32m   3480\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3481\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3483\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[0;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[0;32m   3357\u001b[0m     }\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\table.py:2328\u001b[0m, in \u001b[0;36mtable_cast\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2314\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Improved version of `pa.Table.cast`.\u001b[39;00m\n\u001b[0;32m   2315\u001b[0m \n\u001b[0;32m   2316\u001b[0m \u001b[39mIt supports casting to feature types stored in the schema metadata.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2325\u001b[0m \u001b[39m    table (`pyarrow.Table`): the casted table\u001b[39;00m\n\u001b[0;32m   2326\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2327\u001b[0m \u001b[39mif\u001b[39;00m table\u001b[39m.\u001b[39mschema \u001b[39m!=\u001b[39m schema:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[39mreturn\u001b[39;00m cast_table_to_schema(table, schema)\n\u001b[0;32m   2329\u001b[0m \u001b[39melif\u001b[39;00m table\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mmetadata \u001b[39m!=\u001b[39m schema\u001b[39m.\u001b[39mmetadata:\n\u001b[0;32m   2330\u001b[0m     \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39mreplace_schema_metadata(schema\u001b[39m.\u001b[39mmetadata)\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\table.py:2287\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msorted\u001b[39m(table\u001b[39m.\u001b[39mcolumn_names) \u001b[39m!=\u001b[39m \u001b[39msorted\u001b[39m(features):\n\u001b[0;32m   2286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtable\u001b[39m.\u001b[39mschema\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbecause column names don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2287\u001b[0m arrays \u001b[39m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[39mfor\u001b[39;00m name, feature \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m   2288\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\table.py:2287\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msorted\u001b[39m(table\u001b[39m.\u001b[39mcolumn_names) \u001b[39m!=\u001b[39m \u001b[39msorted\u001b[39m(features):\n\u001b[0;32m   2286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtable\u001b[39m.\u001b[39mschema\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbecause column names don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2287\u001b[0m arrays \u001b[39m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[39mfor\u001b[39;00m name, feature \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m   2288\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\table.py:1831\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1829\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1830\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(array, pa\u001b[39m.\u001b[39mChunkedArray):\n\u001b[1;32m-> 1831\u001b[0m         \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1832\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1833\u001b[0m         \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\table.py:1831\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1829\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1830\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(array, pa\u001b[39m.\u001b[39mChunkedArray):\n\u001b[1;32m-> 1831\u001b[0m         \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1832\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1833\u001b[0m         \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\table.py:2095\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[0;32m   2093\u001b[0m         \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mFixedSizeListArray\u001b[39m.\u001b[39mfrom_arrays(_c(array\u001b[39m.\u001b[39mvalues, feature\u001b[39m.\u001b[39mfeature), feature\u001b[39m.\u001b[39mlength)\n\u001b[0;32m   2094\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2095\u001b[0m     casted_values \u001b[39m=\u001b[39m _c(array\u001b[39m.\u001b[39;49mvalues, feature\u001b[39m.\u001b[39;49mfeature)\n\u001b[0;32m   2096\u001b[0m     \u001b[39mif\u001b[39;00m casted_values\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m array\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtype:\n\u001b[0;32m   2097\u001b[0m         \u001b[39mreturn\u001b[39;00m array\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\table.py:1833\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1833\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\table.py:2063\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[0;32m   2061\u001b[0m     array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mstorage\n\u001b[0;32m   2062\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(feature, \u001b[39m\"\u001b[39m\u001b[39mcast_storage\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 2063\u001b[0m     \u001b[39mreturn\u001b[39;00m feature\u001b[39m.\u001b[39;49mcast_storage(array)\n\u001b[0;32m   2064\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_struct(array\u001b[39m.\u001b[39mtype):\n\u001b[0;32m   2065\u001b[0m     \u001b[39m# feature must be a dict or Sequence(subfeatures_dict)\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, Sequence) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(feature\u001b[39m.\u001b[39mfeature, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\features\\features.py:1104\u001b[0m, in \u001b[0;36mClassLabel.cast_storage\u001b[1;34m(self, storage)\u001b[0m\n\u001b[0;32m   1099\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1100\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClass label \u001b[39m\u001b[39m{\u001b[39;00mmin_max[\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m greater than configured num_classes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1101\u001b[0m         )\n\u001b[0;32m   1102\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(storage, pa\u001b[39m.\u001b[39mStringArray):\n\u001b[0;32m   1103\u001b[0m     storage \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39marray(\n\u001b[1;32m-> 1104\u001b[0m         [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strval2int(label) \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m storage\u001b[39m.\u001b[39mto_pylist()]\n\u001b[0;32m   1105\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[39mreturn\u001b[39;00m array_cast(storage, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_type)\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\features\\features.py:1104\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1099\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1100\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClass label \u001b[39m\u001b[39m{\u001b[39;00mmin_max[\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m greater than configured num_classes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1101\u001b[0m         )\n\u001b[0;32m   1102\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(storage, pa\u001b[39m.\u001b[39mStringArray):\n\u001b[0;32m   1103\u001b[0m     storage \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39marray(\n\u001b[1;32m-> 1104\u001b[0m         [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strval2int(label) \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m storage\u001b[39m.\u001b[39mto_pylist()]\n\u001b[0;32m   1105\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[39mreturn\u001b[39;00m array_cast(storage, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_type)\n",
      "File \u001b[1;32md:\\projects\\postag-thai\\.venv\\lib\\site-packages\\datasets\\features\\features.py:1033\u001b[0m, in \u001b[0;36mClassLabel._strval2int\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   1031\u001b[0m                 failed_parse \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m failed_parse:\n\u001b[1;32m-> 1033\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid string class label \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1034\u001b[0m \u001b[39mreturn\u001b[39;00m int_value\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid string class label NN"
     ]
    }
   ],
   "source": [
    "from datasets import ClassLabel,Sequence\n",
    "\n",
    "pos_tag_thai['train'].cast_column(\"pos_tag\", Sequence(ClassLabel(names=[1, 2, 3,4,5,6,7,8,9,10,11,12,13,14,15,16])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None),\n",
       " 'pos_tag': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'token': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_thai['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_pos_tag = pos_tag_thai['train'].train_test_split(test_size=0.1)\n",
    "#valid_pos = train_test_pos_tag['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "pos_tag_thai = DatasetDict({\n",
    "    'train': train_test_pos_tag['train'],\n",
    "    #'test': valid_pos['train'],\n",
    "    'valid': train_test_pos_tag['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'pos_tag', 'token'],\n",
       "        num_rows: 94003\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'pos_tag', 'token'],\n",
       "        num_rows: 10445\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-th-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [11, 166, 2930, 8349, 12, 188, 5518, 3552, 12], 'token_type_ids': [0, 0, 0, 0, 0, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [11, 142, 5724, 3552, 12, 159, 7662, 8484, 6910, 3552, 4953, 12]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"ภาษา\",\"ไทย\",\"ง่าย\",\"นิดเดียว\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['คดี', 'นี้', 'ผู้', 'ร้อง', 'มี', 'นาย', 'แก้วสรร', 'อติโพธิ', 'นาย', 'สัก', 'กอแสงเรือง']\n"
     ]
    }
   ],
   "source": [
    "example = pos_tag_thai[\"train\"][4]\n",
    "print(example[\"token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ค', '##ดี', 'น', '##ี', '##้', 'ผ', '##ู', '##้', 'ร', '##้อง', 'ม', '##ี', 'น', '##าย', 'แ', '##ก', '##้', '##ว', '##ส', '##ร', '##ร', 'อ', '##ติ', '##โ', '##พ', '##ธ', '##ิ', 'น', '##าย', 'ส', '##ัก', 'ก', '##อ', '##แ', '##ส', '##ง', '##เ', '##ร', '##ือ', '##ง', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"token\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[f\"pos_tag\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 42\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"pos_tag\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"token\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"pos_tag\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 104448/104448 [00:21<00:00, 4858.05 examples/s]\n",
      "Map: 100%|██████████| 13056/13056 [00:02<00:00, 5035.96 examples/s]\n",
      "Map: 100%|██████████| 13056/13056 [00:02<00:00, 5272.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = pos_tag_thai.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Geotrend/bert-base-th-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Geotrend/bert-base-th-cased\", num_labels=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Geotrend/bert-base-th-cased\"\n",
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-pos_thai\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\postag-thai\\.venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 9 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "d:\\projects\\postag-thai\\.venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 8 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "d:\\projects\\postag-thai\\.venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 7 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "d:\\projects\\postag-thai\\.venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 13 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'3': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " '_': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "\n",
    "labels = [label_list[i] for i in example[f\"pos_tag\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
