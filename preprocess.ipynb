{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path):\n",
    "    _data = []\n",
    "    with open(path,'r',encoding=\"utf-8-sig\") as f:\n",
    "        _temp = [i for i in f.read().split(\"\\n\\n\") if i != '']\n",
    "    for j in _temp:\n",
    "        _temp_data = []\n",
    "        bad_data = False\n",
    "        for i in j.split(\"\\n\"):\n",
    "            if i.startswith('#') == False and not bad_data:\n",
    "                _t = i.strip().split(\"\\t\")\n",
    "                if _t[1]!=\"$$\":\n",
    "                    if _t[3]==\"N\":\n",
    "                        _temp_data=[]\n",
    "                        bad_data=True\n",
    "                    else:\n",
    "                        _temp_data.append((_t[1],_t[3]))\n",
    "        if _temp_data!=[]:\n",
    "            _data.append(_temp_data)\n",
    "    return _data\n",
    "\n",
    "def readfile2(path):\n",
    "    _data = []\n",
    "    with open(path,'r',encoding=\"utf-8-sig\") as f:\n",
    "        _temp = [i for i in f.read().split(\"\\n\\n\") if i != '']\n",
    "    for j in _temp:\n",
    "        _temp_data = []\n",
    "        bad_data = False\n",
    "        for i in j.split(\"\\n\"):\n",
    "            if i.startswith('#') == False and not bad_data:\n",
    "                _t = i.strip().split(\"\\t\")\n",
    "                if _t[1]!=\"$$\":\n",
    "                    if _t[3]==\"N\":\n",
    "                        _temp_data=[]\n",
    "                        bad_data=True\n",
    "                    else:\n",
    "                        _temp_data.append((_t[1],_t[4]))\n",
    "        if _temp_data!=[]:\n",
    "            _data.append(_temp_data)\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['section_00.conll', 'section_01.conll', 'section_02.conll', 'section_03.conll', 'section_04.conll', 'section_05.conll', 'section_06.conll', 'section_07.conll', 'section_08.conll', 'section_09.conll', 'section_10.conll', 'section_11.conll', 'section_12.conll', 'section_13.conll', 'section_14.conll', 'section_15.conll', 'section_16.conll', 'section_17.conll', 'section_18.conll', 'section_19.conll', 'section_20.conll', 'section_21.conll', 'section_22.conll', 'section_23.conll', 'section_24.conll']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \"/home/taeta181/Desktop/postag/postag-thai/thai10\"\n",
    "path2 = 'D:/projects/postag-thai/thai10'\n",
    "train_files = os.listdir(path2)\n",
    "print(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in train_files:\n",
    "    train_data.extend(readfile(os.path.join(path2,file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130560"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('สุรยุทธ์', 'NN'), ('ยัน', 'VV'), ('ปฏิเสธ', 'VV'), ('ลงนาม', 'VV'), ('MOU', 'NN'), ('กับ', 'PS'), ('อียู', 'NN'), ('ไม่', 'NG'), ('กระทบ', 'VV'), ('สัมพันธ์', 'NN')]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(len(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PU', 'PS', 'PA', 'CL', 'NU', 'CC', 'PR', 'FX', 'AV', 'AJ', 'XX', 'AX', 'NN', 'IJ', 'VV', 'NG'}\n"
     ]
    }
   ],
   "source": [
    "tag = set()\n",
    "\n",
    "for id in range(len(train_data)):\n",
    "    #print(train_data[id][0][1])\n",
    "    for length_word in range(len(train_data[id])):\n",
    "        tag.add(train_data[id][length_word][1])\n",
    "\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2 = []\n",
    "train_data_2.extend(readfile2(\"th_pud-ud-test.conllu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\projects\\postag-thai\\preprocess.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/projects/postag-thai/preprocess.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(train_data_2))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/postag-thai/preprocess.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(train_data_2[\u001b[39m5\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_2' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(train_data_2))\n",
    "print(train_data_2[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VV', 'NNP', 'CD', 'RB', 'IN', 'AS', 'JJ', 'NNB', 'VC', 'DT', 'MD', 'NEG', 'NN', 'WP', 'CC', 'PRD', '``', 'PRP'}\n"
     ]
    }
   ],
   "source": [
    "tag2 = set()\n",
    "\n",
    "for id in range(len(train_data_2)):\n",
    "    #print(train_data[id][0][1])\n",
    "    tag2.add(train_data_2[id][0][1])\n",
    "\n",
    "print(tag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "def save_pos_data (data, filename):\n",
    "    data_collect = pd.DataFrame(columns = ['id','pos_tag','token'])\n",
    "    for i in tqdm(range(len(data))):\n",
    "        pos_temp = []\n",
    "        token_temp = []\n",
    "        for j in range(len(data[i])):\n",
    "            pos_temp.append(data[i][j][1])\n",
    "            token_temp.append(data[i][j][0])\n",
    "        \n",
    "        data_collect.loc[i] = [i,pos_temp,token_temp]\n",
    "    filename = filename + \".json\"\n",
    "    data_collect.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1538.75it/s]\n"
     ]
    }
   ],
   "source": [
    "save_pos_data(train_data_2,\"th_pud_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PR': 1, 'PS': 2, 'NU': 3, 'AV': 4, 'XX': 5, 'FX': 6, 'AJ': 7, 'NN': 8, 'AX': 9, 'PU': 10, 'IJ': 11, 'VV': 12, 'CC': 13, 'CL': 14, 'PA': 15, 'NG': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\postag-thai\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "import pickle\n",
    "\n",
    "# Read dictionary pkl file\n",
    "with open('tag_id.pkl', 'rb') as fp:\n",
    "    tag_id_thai10 = pickle.load(fp)\n",
    "    print(tag_id_thai10)\n",
    "\n",
    "def save_pos_data_json (data, filename):\n",
    "    data_collect = pd.DataFrame(columns = ['id','pos_tag','token'])\n",
    "    for i in tqdm(range(len(data))):\n",
    "        pos_temp = []\n",
    "        token_temp = []\n",
    "        for j in range(len(data[i])):\n",
    "            #tag_id_str_to_int = tag_id_thai10[data[i][j][1]] \n",
    "            pos_temp.append(data[i][j][1])\n",
    "            token_temp.append(data[i][j][0])\n",
    "        \n",
    "        data_collect.loc[i] = [i,pos_temp,token_temp]\n",
    "    filename = filename + \".json\"\n",
    "\n",
    "    data_collect.to_json(filename, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 960.51it/s]\n"
     ]
    }
   ],
   "source": [
    "save_pos_data_json(train_data_2, \"test_pud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130560/130560 [18:20<00:00, 118.60it/s]\n"
     ]
    }
   ],
   "source": [
    "save_pos_data_json(train_data, \"thai10_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag id\n",
      "{'PR': 1, 'PS': 2, 'NU': 3, 'AV': 4, 'XX': 5, 'FX': 6, 'AJ': 7, 'NN': 8, 'AX': 9, 'PU': 10, 'IJ': 11, 'VV': 12, 'CC': 13, 'CL': 14, 'PA': 15, 'NG': 16}\n",
      "dictionary saved successfully to file\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# create a dictionary using {}\n",
    "tag_id = {'PR' : 1, 'PS' : 2, 'NU' :3, 'AV':4, 'XX':5, 'FX':6, 'AJ':7, 'NN':8, 'AX':9, 'PU':10, 'IJ':11, 'VV':12, 'CC':13, 'CL':14, 'PA':15, 'NG':16}\n",
    "print('tag id')\n",
    "print(tag_id)\n",
    "\n",
    "# save dictionary to person_data.pkl file\n",
    "with open('tag_id.pkl', 'wb') as fp:\n",
    "    pickle.dump(tag_id, fp)\n",
    "    print('dictionary saved successfully to file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
