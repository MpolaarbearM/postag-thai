{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path):\n",
    "    _data = []\n",
    "    with open(path,'r',encoding=\"utf-8-sig\") as f:\n",
    "        _temp = [i for i in f.read().split(\"\\n\\n\") if i != '']\n",
    "    for j in _temp:\n",
    "        _temp_data = []\n",
    "        bad_data = False\n",
    "        for i in j.split(\"\\n\"):\n",
    "            if i.startswith('#') == False and not bad_data:\n",
    "                _t = i.strip().split(\"\\t\")\n",
    "                if _t[1]!=\"$$\":\n",
    "                    if _t[3]==\"N\":\n",
    "                        _temp_data=[]\n",
    "                        bad_data=True\n",
    "                    else:\n",
    "                        _temp_data.append((_t[1],_t[3]))\n",
    "        if _temp_data!=[]:\n",
    "            _data.append(_temp_data)\n",
    "    return _data\n",
    "\n",
    "def readfile2(path):\n",
    "    _data = []\n",
    "    with open(path,'r',encoding=\"utf-8-sig\") as f:\n",
    "        _temp = [i for i in f.read().split(\"\\n\\n\") if i != '']\n",
    "    for j in _temp:\n",
    "        _temp_data = []\n",
    "        bad_data = False\n",
    "        for i in j.split(\"\\n\"):\n",
    "            if i.startswith('#') == False and not bad_data:\n",
    "                _t = i.strip().split(\"\\t\")\n",
    "                if _t[1]!=\"$$\":\n",
    "                    if _t[3]==\"N\":\n",
    "                        _temp_data=[]\n",
    "                        bad_data=True\n",
    "                    else:\n",
    "                        _temp_data.append((_t[1],_t[4]))\n",
    "        if _temp_data!=[]:\n",
    "            _data.append(_temp_data)\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['section_14.conll', 'section_18.conll', 'section_23.conll', 'section_02.conll', 'section_15.conll', 'section_19.conll', 'section_22.conll', 'section_03.conll', 'section_16.conll', 'section_00.conll', 'section_21.conll', 'section_17.conll', 'section_01.conll', 'section_20.conll', 'section_10.conll', 'section_06.conll', 'section_11.conll', 'section_07.conll', 'section_12.conll', 'section_08.conll', 'section_04.conll', 'section_13.conll', 'section_09.conll', 'section_05.conll', 'section_24.conll']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \"/home/taeta181/Desktop/postag/postag-thai/thai10\"\n",
    "\n",
    "train_files = os.listdir(path)\n",
    "print(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in train_files:\n",
    "    train_data.extend(readfile(os.path.join(path,file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ก่อน', 'CC'),\n",
       " ('ที่', 'CC'),\n",
       " ('เครื่อง', 'NN'),\n",
       " ('บิน', 'VV'),\n",
       " ('จะ', 'AX'),\n",
       " ('สูญหาย', 'VV'),\n",
       " ('ไป', 'AV')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('และ', 'CC'), ('หา', 'VV'), ('ยาก', 'VV'), ('ที่', 'CC'), ('ฟ้า', 'NN'), ('ผ่า', 'VV'), ('จะ', 'AX')]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AV', 'FX', 'IJ', 'PS', 'CL', 'CC', 'NG', 'VV', 'AJ', 'AX', 'NN', 'PR', 'NU'}\n"
     ]
    }
   ],
   "source": [
    "tag = set()\n",
    "\n",
    "for id in range(len(train_data)):\n",
    "    #print(train_data[id][0][1])\n",
    "    tag.add(train_data[id][0][1])\n",
    "\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2 = []\n",
    "train_data_2.extend(readfile2(\"th_pud-ud-test.conllu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[('สิ่ง', 'NN'), ('ที่', 'WDT'), ('เธอ', 'PRP'), ('กำลัง', 'AS'), ('พูด', 'VV'), ('และ', 'CC'), ('สิ่ง', 'NN'), ('ที่', 'WDT'), ('เธอ', 'PRP'), ('กำลัง', 'AS'), ('ทำ', 'VV'), ('มัน', 'PRP'), ('—', '-'), ('ที่', 'PRP'), ('จริง', 'JJ'), ('มัน', 'PRP'), ('ไม่', 'NEG'), ('น่าเชื่อ', 'VV'), ('เลย', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_2))\n",
    "print(train_data_2[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CD', 'RB', 'IN', 'NNP', '``', 'DT', 'JJ', 'PRP', 'PRD', 'NNB', 'VC', 'MD', 'CC', 'NEG', 'VV', 'WP', 'NN', 'AS'}\n"
     ]
    }
   ],
   "source": [
    "tag2 = set()\n",
    "\n",
    "for id in range(len(train_data_2)):\n",
    "    #print(train_data[id][0][1])\n",
    "    tag2.add(train_data_2[id][0][1])\n",
    "\n",
    "print(tag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "def save_pos_data (data, filename):\n",
    "    data_collect = pd.DataFrame(columns = ['id','pos_tag','token'])\n",
    "    for i in tqdm(range(len(data))):\n",
    "        pos_temp = []\n",
    "        token_temp = []\n",
    "        for j in range(len(data[i])):\n",
    "            pos_temp.append(data[i][j][1])\n",
    "            token_temp.append(data[i][j][0])\n",
    "        \n",
    "        data_collect.loc[i] = [i,pos_temp,token_temp]\n",
    "    filename = filename + \".json\"\n",
    "    data_collect.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1538.75it/s]\n"
     ]
    }
   ],
   "source": [
    "save_pos_data(train_data_2,\"th_pud_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
